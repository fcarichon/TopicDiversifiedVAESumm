from collections import defaultdict
import logging
import os
import pdb
from copy import copy
from nltk.corpus import stopwords
import spacy
import numpy as np
from rouge_score import rouge_scorer

class Rouge_eval(object):

    def __init__(self, remove_stopwords=True, use_stemmer=True):
        self.remove_stopwords = remove_stopwords
        if remove_stopwords:
            self.stopwords = set(stopwords.words('english'))
        self.use_stemmer = use_stemmer
        self.nlp = spacy.load("en_core_web_sm")
        # python implementation of ROUGE
        self.rouge_scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=use_stemmer)
        
        #Create dictionnary for storing final scores
        self.final_results = {}
    
    def calc_rouges(self, ref, summ):
        """Calling rouge scorer function"""
        if self.remove_stopwords:
            ref = ' '.join([str(word) for word in self.nlp(str(ref)) if str(word) not in self.stopwords])
            summ = ' '.join([str(word) for word in self.nlp(str(summ[0])) if str(word) not in self.stopwords])
        else:
            summ = summ[0]
        
        rouge_scores = self.rouge_scorer.score(ref, summ)
        
        return rouge_scores
        
    def get_rouge_defaultdict(self, default_type=float, store_summ=False):
        """Initializing storing dictionary for each individual results"""
        if store_summ:
            dict_ = {'text' : None, 
                    'rouge1': defaultdict(default_type),
                    'rouge2': defaultdict(default_type),
                    'rougeL': defaultdict(default_type)}
        else:
            dict_ = {'rouge1': defaultdict(default_type),
                     'rouge2': defaultdict(default_type),
                     'rougeL': defaultdict(default_type)}
        return dict_
    
    def rouge_results(self, references, generated_summaries, group_id, list_metric=['precision', 'recall', 'fmeasure']):
        
        """Computin the ROUGE results for all N summaries with N references.
        Args:
            references: Human labeled references
            generated_summaries: Generated summaries by the model
            group_id: Batch ID
            list_metric: Metric to compute with ROUGE
        Output:
            self.final_results: Final results dictionnary
        """
        
        #Create dictionnary that is going to store results for the group
        rouges = {}
        batch_result = {}
        
        #We go through the N summaries generated by the model - 1 output by topic
        for i, summ in enumerate(generated_summaries):
            rouges[i] = self.get_rouge_defaultdict(default_type=list, store_summ=True)
            rouges[i]['text'] = summ
            for ref in references:
                rouge_scores = self.calc_rouges(ref, summ)
                for rouge_name, rouge_obj in rouge_scores.items():
                    for metric in list_metric:
                        score = getattr(rouge_obj, metric)
                        #print('score : ', score)
                        rouges[i][rouge_name][metric].append(score)
                        
                        
            # Compute statistics for each summary
            avg_rouges = self.get_rouge_defaultdict()
            min_rouges = self.get_rouge_defaultdict()
            max_rouges = self.get_rouge_defaultdict()
            std_rouges = self.get_rouge_defaultdict()
            
            for rouge_name, rouge_obj in rouges[i].items():
                if rouge_name == 'text':
                    pass
                else:
                    for metric in list_metric:
                        scores = rouges[i][rouge_name][metric]
                        avg_, min_, max_, std_ = np.mean(scores), np.min(scores), np.max(scores), np.std(scores)
                        avg_rouges[rouge_name][metric] = avg_
                        min_rouges[rouge_name][metric] = min_
                        max_rouges[rouge_name][metric] = max_
                        std_rouges[rouge_name][metric] = std_
            

            batch_result['gensumm_{}'.format(i)] = {'text': summ, 'references': references, 
                                            'avg': avg_rouges, 'min': min_rouges, 
                                            'max' : max_rouges, 'std' : std_rouges}    
        
        self.final_results[group_id] = batch_result
        

def get_rouge_average(rouge_results):
    """knowing computation of all rouge scores - get avg for batch
    Args: 
        rouge_results == self.final_results - dictionnary of results
    Output:
        dict_result: Dictionnary with all metrics averaged"""
    
    counter=0
    avg_avg_1, avg_avg_2, avg_avg_L, avg_max_1, avg_max_2, avg_max_L = 0, 0, 0, 0, 0, 0
    for i in rouge_results:
        for summ_ in rouge_results[i]:
            avg_avg_1 += rouge_results[i][summ_]['avg']['rouge1']['fmeasure']
            avg_avg_L += rouge_results[i][summ_]['avg']['rougeL']['fmeasure']
            avg_avg_2 +=  rouge_results[i][summ_]['avg']['rouge2']['fmeasure']
            avg_max_2 += rouge_results[i][summ_]['max']['rouge2']['fmeasure']
            avg_max_1 += rouge_results[i][summ_]['max']['rouge1']['fmeasure']
            avg_max_L += rouge_results[i][summ_]['max']['rougeL']['fmeasure']
            counter += 1
    
    dict_result = {'avg_avg_1': avg_avg_1 / counter,
                   'avg_avg_L': avg_avg_L / counter,
                   'avg_avg_2': avg_avg_2 / counter,
                   'avg_max_2': avg_max_2 / counter,
                   'avg_max_1': avg_max_1 / counter,
                   'avg_max_L': avg_max_L / counter}
    
    return dict_result
    
def get_rouge_max (rouge_results):
    
    """knowing computation of all rouge scores - get max scores of the 3 summaries for batch
    Args: 
        rouge_results == self.final_results - dictionnary of results
    Output:
        dict_result: Dictionnary with all metrics maxed with the best matching pair"""
    
    max_avg_1, max_avg_L, max_avg_2, max_max_2, max_max_1, max_max_L = 0, 0, 0, 0, 0, 0
    for i in rouge_results:
        avg_1, avg_L, avg_2, max_2, max_1, max_L = 0, 0, 0, 0, 0, 0
        for summ_ in rouge_results[i]:
            if rouge_results[i][summ_]['avg']['rouge1']['fmeasure'] > avg_1:
                avg_1 = rouge_results[i][summ_]['avg']['rouge1']['fmeasure']
            if rouge_results[i][summ_]['avg']['rougeL']['fmeasure'] > avg_L:
                avg_L = rouge_results[i][summ_]['avg']['rougeL']['fmeasure']
            if rouge_results[i][summ_]['avg']['rouge2']['fmeasure'] > avg_2:
                avg_2 = rouge_results[i][summ_]['avg']['rouge2']['fmeasure']
            if rouge_results[i][summ_]['max']['rouge2']['fmeasure'] > max_2:
                max_2 = rouge_results[i][summ_]['max']['rouge2']['fmeasure']
            if rouge_results[i][summ_]['max']['rouge1']['fmeasure'] > max_1:
                max_1 = rouge_results[i][summ_]['max']['rouge1']['fmeasure']
            if rouge_results[i][summ_]['max']['rougeL']['fmeasure'] > max_L:
                max_L = rouge_results[i][summ_]['max']['rougeL']['fmeasure']

        max_avg_1 += avg_1
        max_avg_L += avg_L
        max_avg_2 += avg_2
        max_max_2 += max_2
        max_max_1 += max_1
        max_max_L += max_L
                
    dict_result = {'max_avg_1': max_avg_1 / len(rouge_results),
                   'max_avg_L': max_avg_L / len(rouge_results),
                   'max_avg_2': max_avg_2 / len(rouge_results),
                   'max_max_2': max_max_2 / len(rouge_results),
                   'max_max_1': max_max_1 / len(rouge_results),
                   'max_max_L': max_max_L / len(rouge_results)}
    
    return dict_result